#  LOOP MACHINE

Browser-based trance synthesizer with reactive 3D visuals, AI-powered audio generation, and LoRA fine-tuning.

## Features

### Sequencer & Sound Engine
- **8-Instrument Step Sequencer** — 16-step grid at 16th-note resolution with kick, bass, hi-hat, clap, acid, lead, perc, and stab tracks
- **Multi-Bar Patterns** — 1-4 bar loops with per-bar editing and tab navigation
- **Multi-Hit Steps** — Cycle through 1/2/3 hits per step for ratchets and drum rolls
- **Per-Instrument States** — Normal, force (always plays), and mute modes per track
- **Pattern Styles** — Multiple algorithmic styles per instrument (e.g. bass: rolling, galloping, syncopated, offbeat, pulsing)
- **Musical Scales** — 6 scales (Phrygian, Natural Minor, Harmonic Minor, Dorian, Locrian, Hungarian Minor) and 8 root notes
- **Randomize** — One-click full pattern regeneration with new key, scale, and styles

### Sound Design
- **Master Controls** — Intensity, filter cutoff (60Hz-20kHz), swing, distortion
- **Synth Parameters** — Bass decay, acid resonance, acid LFO rate, reverb mix, delay mix, kick punch
- **Audio Chain** — Filter → Distortion → Compressor → Limiter → Gain
- **Per-Instrument Synthesis** — MembraneSynth (kick), MonoSynth with portamento (bass), resonant square wave with LFO (acid), PolySynth with delay/reverb (lead), MetalSynth (hats), NoiseSynth (claps/perc)

### Song Arranger
- **Arrangement Sequencing** — Chain bars in any order for song-form playback
- **Loop vs Song Mode** — Toggle between sequential loop and custom arrangement
- **Bridge Generation** — 4 algorithmic transition types:
  - **Build-up** — Accelerating hats, rising perc, riser, kick dropout
  - **Breakdown** — Minimal acid + stabs only
  - **Fill** — Dense syncopated drum fill
  - **Drop** — Full energy with all instruments active
- **Block Management** — Add, copy, remove blocks; visual position tracking during playback

### AI Audio Generation (ACE-Step 1.5)
- **Text-to-Music** — Generate audio clips up to 240 seconds from text prompts
- **Prompt Preview** — Review and edit the full prompt before sending to ACE-Step
- **Generation Controls** — Duration, inference steps (4/8/16/32), guidance scale (1-25)
- **Vocal Mode** — Toggle instrumental vs vocal with lyrics input
- **BPM Sync** — Auto-passes current synth BPM to the generation model
- **Auto-Download** — Generated clips are automatically added to the sample bank

### LLM Tag Cascade
- **Dynamic Tags** — 8 categories (subgenre, instruments, mood, characteristics, timbre, era, production, region) generated by a local LLM based on selected genres
- **12 Genre Seeds** — Psytrance, darkpsy, goa trance, progressive, hi-tech, forest, full-on, techno, ambient, drum & bass, house, experimental
- **Multi-Select Genres** — Select multiple genres; each adds to the prompt without clearing others
- **Custom Text** — Type additional text after auto-generated tags; preserved across tag clicks
- **Fallback Mode** — Hardcoded tags displayed if the LLM is offline

### Sample Management
- **Sample Bank** — Generated clips stored in IndexedDB with preview, edit, download, delete
- **4 Sample Slots** — Color-coded (orange/green/blue/magenta) looping players synced to transport
- **Sample Mixer** — Per-slot volume (-40 to +6 dB), mute, loop toggle, clear
- **Clip Editor** — Waveform display with start/end trim sliders, preview, save-as-new or replace
- **Sample Library** — Full gallery modal with rename, re-order, slot assignment, download

### Beat Maps
- **Save/Load** — Persist complete synth state (patterns, hits, instrument states, BPM, key, scale, arrangement) to IndexedDB
- **Map Browser** — List saved maps with timestamps, click to load, delete

### AI Textures (Google Gemini)
- **Texture Generation** — Create psychedelic textures via Google Gemini image API
- **4 Presets** — Sacred geometry, organic bioluminescent, fractal electric, cyberpunk circuitry
- **Application Modes** — Apply to 3D tunnel background or geometry surfaces (triplanar mapping)
- **Gallery** — Thumbnail grid of generated textures with click-to-apply, persistent in IndexedDB

### 3D Visualization
- **Reactive Geometry** — Central morphing icosahedron (solid + wireframe) with custom displacement shaders
- **Orbiting Toroids** — 5 ring shapes with dynamic audio-responsive scaling
- **Particle System** — 3000 particles with frequency-band color modulation
- **Tunnel Background** — Spherical tunnel with radial patterns and pulsing textures
- **Post-Processing** — UnrealBloom, 6-sided kaleidoscope shader, chromatic aberration (spikes on kick)
- **4 Color Palettes** — Auto-cycling every 8 seconds with smooth interpolation
- **Camera Controls** — Mouse-drag orbit, right-drag dolly zoom, auto-sway oscillation

### LoRA Studio
- **Adapter Management** — Load, unload, toggle, and scale LoRA adapters for ACE-Step's DiT decoder
- **Adapter Browser** — Scan output directory for trained adapters, click to select
- **Training Pipeline** — Preprocess audio → train LoRA → use for generation (see [LoRA Training Guide](#lora-training-guide) below)
- **Training Controls** — Configurable rank, alpha, epochs, learning rate with real-time progress monitoring

---

## Quick Start

```bash
# Start the web server
python3 server.py 8080
```

Open `http://localhost:8080` in your browser. The sequencer and 3D visuals work standalone.

### With AI Audio Generation

Run [ACE-Step 1.5](https://github.com/ace-step/ACE-Step) on port 8001:

```bash
cd /path/to/ACE-Step-1.5
uv run acestep-api --init-llm --lm-model-path acestep-5Hz-lm-4B
```

The proxy server forwards `/ace-api/*` automatically:

```bash
ACE_STEP_URL=http://localhost:8001 python3 server.py 8080
```

### With LLM Tag Generation

Serve a local LLM via vLLM on port 8002 for dynamic tag cascade:

```bash
bash start_llm.sh
```

This serves an 8B parameter model (~17-20 GB VRAM). Without it, fallback hardcoded tags are shown.

---

## Configuration

Copy `.env.example` to `.env` and add your API keys:

```bash
cp .env.example .env
```

| Variable | Purpose | Default |
|----------|---------|---------|
| `GOOGLE_API_KEY` | Google Gemini API key for texture generation | — |
| `ACE_STEP_URL` | ACE-Step server URL | `http://localhost:8001` |
| `LLM_URL` | vLLM server URL for tag generation | `http://localhost:8002` |

---

## LoRA Training Guide

The LoRA Studio lets you fine-tune ACE-Step's DiT decoder on your own audio to create custom sound adapters. Here's the complete workflow.

### How LoRA Works

LoRA (Low-Rank Adaptation) injects small trainable matrices into the model's attention layers (q/k/v/o projections) while keeping the base model frozen. This means:

- **Tiny adapter files** (~5-20 MB vs 6+ GB base model)
- **Fast training** (minutes to hours, not days)
- **No base model modification** — load/unload adapters freely
- **Stackable influence** — scale from 0% to 100% effect

### Preparing Your Dataset

**Supported formats:** `.wav`, `.mp3`, `.flac`, `.ogg`, `.opus`

**Directory structure — just put audio files in a folder:**

```
my_dataset/
├── track1.wav
├── track2.mp3
├── ambient_pad.flac
├── bassline_loop.ogg
└── ...
```

**Optional enhancements:**

- **Companion lyrics:** Place a `.txt` file with the same name next to an audio file to provide lyrics
  ```
  my_dataset/
  ├── vocal_track.wav
  ├── vocal_track.txt    ← lyrics for vocal_track.wav
  ```
- **CSV metadata:** Place a CSV file in the directory with columns for `filename`, `bpm`, `keyscale`, `caption` to pre-fill metadata (avoids LLM auto-labeling for those fields)

**Best practices:**
- 5-50 audio files is a good range for LoRA training
- Consistent style/genre across files produces more coherent adapters
- Files can be any length (up to 240 seconds; longer files are truncated)
- Stereo or mono both work (mono is auto-duplicated to stereo)
- 48kHz is native sample rate (others are resampled automatically)

### Step 1: Preprocessing

Click **PREPROCESS** in the LoRA Studio after entering your audio directory path. This runs three stages:

**1a. Scan** — Walks the directory tree, discovers audio files, loads companion lyrics/CSV metadata, calculates durations.

**1b. Auto-Label** — For each audio file, the system:
1. Encodes the audio into semantic tokens via the DiT model's audio encoder
2. Sends tokens to the 5Hz LLM which generates:
   - **Caption** — Natural language description ("dark rolling psytrance bassline with squelchy 303 acid")
   - **Genre tags** — Style classifications
   - **BPM** — Estimated beats per minute
   - **Key/Scale** — Detected musical key
   - **Time signature** — 4/4, 3/4, etc.
   - **Language** — Vocal language (or "instrumental")
   - **Lyrics** — Transcribed lyrics (if vocal)

   CSV-provided metadata is preserved and not overwritten.

**1c. Tensor Conversion** — For each labeled sample:
1. Load audio at 48kHz stereo
2. Encode through VAE → `target_latents` (compressed audio representation)
3. Tokenize caption + metadata → `text_hidden_states` (text condition)
4. Tokenize lyrics → `lyric_hidden_states` (lyric condition)
5. Fuse conditions through the model encoder → `encoder_hidden_states`
6. Build context latents from silence template → `context_latents`
7. Save all tensors to a `.pt` file (~1-10 MB per sample)

The tensor directory path auto-fills after preprocessing completes.

**Why preprocess?** All the expensive VAE/text encoding is done once upfront. Training then loads pre-computed tensors directly — no VAE or text encoder needed during training, making each epoch very fast.

### Step 2: Training

Set your parameters and click **START TRAINING**:

| Parameter | Range | Default | Description |
|-----------|-------|---------|-------------|
| **Rank** | 1-64 | 8 | LoRA rank — higher = more capacity but more VRAM |
| **Alpha** | 1-128 | 16 | Scaling factor — `alpha/rank` controls LoRA strength |
| **Epochs** | 1-500 | 100 | Training iterations over the full dataset |
| **LR** | 1e-6 to 1e-2 | 1e-4 | Learning rate (log scale slider) |

**What happens during training:**

1. LoRA matrices are injected into the DiT decoder's attention layers (q_proj, k_proj, v_proj, o_proj)
2. All base model parameters are frozen — only LoRA matrices train
3. Each step:
   - Load pre-computed tensors (target audio latents + conditions)
   - Sample random noise and a discrete timestep from the turbo schedule
   - Interpolate between noise and audio: `x_t = t * noise + (1-t) * audio`
   - Forward through decoder to predict the flow (velocity from audio to noise)
   - Compute MSE loss between predicted and actual flow
   - Update only LoRA parameters via AdamW optimizer
4. Checkpoints saved every N epochs (default 10) with full PEFT adapter format

**Training tips:**
- Start with defaults (rank=8, alpha=16, lr=1e-4, 100 epochs)
- Watch the loss curve — it should decrease and plateau
- Higher rank (16-32) for complex style changes, lower (4-8) for subtle tweaks
- Reduce LR (1e-5) if loss is unstable, increase (5e-4) if learning is too slow
- Training locks the inference executor — generation is paused during training

**Output structure:**
```
lora_output/
└── lora_20250208_143022/
    ├── adapter/
    │   ├── adapter_config.json
    │   └── adapter_model.safetensors
    ├── lora_weights.pt
    └── checkpoints/
        ├── epoch_10/
        ├── epoch_20/
        └── ...
```

### Step 3: Using Your LoRA

1. Click **REFRESH** in the adapter browser to see your trained adapters
2. Click an adapter to fill the path field
3. Click **LOAD** — the status dot turns green
4. Click **ON** to activate — dot turns cyan
5. Adjust **SCALE** slider (0.0 = base model, 1.0 = full LoRA effect)
6. Generate audio as normal — your LoRA influences the output style

**Controls:**
- **ON/OFF** — Toggle LoRA without unloading (instant, uses PEFT enable/disable)
- **SCALE** — Blend between base model and LoRA (useful for subtle influence)
- **UNLOAD** — Fully restore the base model (frees LoRA memory)

You can load different adapters for different generation tasks. The base model is always preserved — unloading restores it exactly.

---

## Architecture

```
Browser (localhost:8080)
├── index.html      — UI, sequencer grid, all controls, LoRA Studio
├── synth.js        — Tone.js audio engine, pattern generation, arrangement
├── visualizer.js   — Three.js 3D scene, shaders, post-processing
├── api.js          — Google Gemini texture API, IndexedDB for textures
└── samples.js      — IndexedDB for samples, blob management

server.py (Python, port 8080)
├── Static file server
├── /ace-api/*  → ACE-Step proxy (port 8001)
└── /llm-api/*  → vLLM proxy (port 8002)

ACE-Step 1.5 (port 8001)
├── Music generation (DiT + VAE + 5Hz LM)
├── LoRA training pipeline
└── 10 LoRA REST endpoints

vLLM (port 8002)
└── Qwen3-8B for tag generation
```

## Tech Stack

- [Tone.js 14.7.77](https://tonejs.github.io/) — Web Audio synthesis
- [Three.js r160](https://threejs.org/) — 3D graphics (ES module)
- [ACE-Step 1.5](https://github.com/ace-step/ACE-Step) — AI music generation with LoRA support
- [Google Gemini](https://ai.google.dev/) — AI image generation for textures
- [vLLM](https://github.com/vllm-project/vllm) — LLM inference for dynamic tag generation
- [PEFT](https://github.com/huggingface/peft) — Parameter-Efficient Fine-Tuning for LoRA adapters

## VRAM Requirements

| Component | VRAM | Notes |
|-----------|------|-------|
| ACE-Step 1.5 (DiT + VAE + 5Hz LM) | ~27 GB | Base inference |
| vLLM (Qwen3-8B) | ~17-20 GB | Tag generation |
| LoRA Training | ~5-15 GB additional | Depends on batch size and rank |
| **Total** | **~50-60 GB** | All services running |

A GPU with 48+ GB VRAM can run inference. Training requires 60+ GB to run alongside inference services.

## License

MIT
